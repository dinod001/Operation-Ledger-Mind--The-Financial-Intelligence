{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f5a310ab",
            "metadata": {},
            "source": [
                "### 1. Import Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "42ce2688",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_groq import ChatGroq\n",
                "from dotenv import load_dotenv\n",
                "import json\n",
                "import os\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9cdeee46",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tesseract path injected: C:\\Program Files\\Tesseract-OCR\n"
                    ]
                }
            ],
            "source": [
                "load_dotenv()\n",
                "\n",
                "# Configure Tesseract path from .env\n",
                "import pytesseract\n",
                "tess_path = os.getenv(\"TESSERACT_PATH\")\n",
                "if tess_path:\n",
                "    # 1. Set path for pytesseract library\n",
                "    pytesseract.pytesseract.tesseract_cmd = tess_path\n",
                "    \n",
                "    # 2. Add Tesseract directory to system PATH for unstructured's subprocess calls\n",
                "    tess_dir = os.path.dirname(tess_path)\n",
                "    if tess_dir not in os.environ['PATH']:\n",
                "        os.environ['PATH'] = tess_dir + os.pathsep + os.environ['PATH']\n",
                "    print(f\"Tesseract path injected: {tess_dir}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a4794cc",
            "metadata": {},
            "source": [
                "### 2. Data Ingestion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "a08d294c",
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path=\"../data/report.pdf\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ef22baed",
            "metadata": {},
            "outputs": [],
            "source": [
                "loader = UnstructuredPDFLoader(\n",
                "    file_path,\n",
                "    strategy=\"hi_res\",  # indentyfying table and layout\n",
                "    infer_table_structure=True, # To get data from inside table\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "3f0e2dc7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Warning: No languages specified, defaulting to English.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
                    ]
                }
            ],
            "source": [
                "docs  =loader.load()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "5994eeac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Characters: 626032\n",
                        "Start: On Our Way  2024 ANNUAL REPORT \n",
                        "\n",
                        "On Our Way\n",
                        "\n",
                        "Uber’s Mission We reimagine the way the world moves for the better\n",
                        "\n",
                        "Uber’s Mission\n",
                        "\n",
                        "We reimagine the way the world moves for the better We are Uber. The go\n",
                        "End:  94158\n",
                        "\n",
                        "PricewaterhouseCoopers LLP\n",
                        "\n",
                        "Independent Public Registered\n",
                        "\n",
                        "Accounting Firm\n",
                        "\n",
                        "PricewaterhouseCoopers LLP\n",
                        "\n",
                        "f\n",
                        "\n",
                        "2021 Proxy statement\n",
                        "\n",
                        "Uber\n",
                        "\n",
                        "1725 3rd Street San Francisco, California 94158\n",
                        "\n",
                        "uber.com\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Characters: {len(docs[0].page_content)}\")\n",
                "print(f\"Start: {docs[0].page_content[:200]}\")\n",
                "print(f\"End: {docs[0].page_content[-200:]}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731450be",
            "metadata": {},
            "source": [
                "### 3. Text Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c70bb7b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size = 1500,\n",
                "    chunk_overlap = 150,\n",
                "    length_function = len,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "997e5cb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "chunks = text_splitter.split_documents(docs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a130845e",
            "metadata": {},
            "source": [
                "#### 3.1 Saving & Loading chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "aaea1fb7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Chunks Saved Sucessfully !\n"
                    ]
                }
            ],
            "source": [
                "chunks_json_path = '../artifacts/data_factory/chunks.json'\n",
                "\n",
                "serializable_chunks = [\n",
                "    {\n",
                "        \"page_content\": doc.page_content, \n",
                "        \"metadata\": doc.metadata\n",
                "    } \n",
                "    for doc in chunks\n",
                "]\n",
                "\n",
                "with open(chunks_json_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(serializable_chunks, f, indent=4)\n",
                "\n",
                "print(\"Chunks Saved Sucessfully !\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "2365faa3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Chunks loaded from JSON file.\n"
                    ]
                }
            ],
            "source": [
                "if os.path.exists(chunks_json_path):\n",
                "    with open(chunks_json_path, 'r', encoding='utf-8') as f:\n",
                "        chunks = json.load(f)\n",
                "    print(\"✅ Chunks loaded from JSON file.\")\n",
                "else:\n",
                "    chunks = text_splitter.split_documents(docs)\n",
                "    print(\"✅ Documents split into new chunks.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "33731fa0",
            "metadata": {},
            "source": [
                "### 4. Question & Answer generator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "a0525c5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
                "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "llm_groq = ChatGroq(model=\"llama-3.1-8b-instant\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71aba377",
            "metadata": {},
            "source": [
                "#### 4.1 Question & Answer generator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "7368fcbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "question_gen_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a Lead AI Architect. Your task is to generate 10 high-quality questions based ONLY on the provided context.\n",
                "\n",
                "    RULES:\n",
                "    1. Every question MUST be answerable using ONLY the provided text.\n",
                "    2. Do NOT use outside knowledge (like general financial stats for Uber).\n",
                "    3. If the context is narrative, ask about goals, tone, or mission.\n",
                "    4. If the context has data, ask about specific numbers and percentages.\n",
                "    5. No intro/outro. Just 10 numbered questions.\"\"\"),\n",
                "    (\"user\", \"Context: {context}\")\n",
                "])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "d571cf20",
            "metadata": {},
            "outputs": [],
            "source": [
                "answer_gen_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a Senior Financial Analyst. \n",
                "    Your task is to answer the provided questions using ONLY the given context.\n",
                "    \n",
                "    You must return the output as a list of Question-Answer pairs.\n",
                "    Format each pair exactly like this:\n",
                "    Q: [The original question]\n",
                "    A: [The concise answer based on context]\n",
                "\n",
                "    Rules:\n",
                "    1. Do not use outside knowledge.\n",
                "    2. No citations or tags.\n",
                "    3. If the answer is not in the context, write \"Information not available\".\n",
                "    4. Provide exactly 10 pairs.\"\"\"),\n",
                "    (\"user\", \"Context: {context}\\n\\nQuestions: {questions}\")\n",
                "])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "508a2bff",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_parser = StrOutputParser()\n",
                "chain_openAI = question_gen_prompt | llm_openai | output_parser\n",
                "chain_gemini = answer_gen_prompt | llm_gemini | output_parser\n",
                "chain_groq = answer_gen_prompt | llm_groq | output_parser\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "34910cde",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " From 560 to 610 processing ...\n",
                        "Saved checkpoint at chunk 570\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "output_file = \"../artifacts/data_factory/uber_dataset.json\"\n",
                "\n",
                "if os.path.exists(output_file):\n",
                "    with open(output_file, \"r\") as f:\n",
                "        dataset = json.load(f)\n",
                "else:\n",
                "    dataset = []\n",
                "\n",
                "\n",
                "start_chunk = len(dataset)\n",
                "end_chunk = start_chunk + 50 \n",
                "\n",
                "print(f\" From {start_chunk} to {end_chunk} processing ...\")\n",
                "\n",
                "for i, chunk in enumerate(chunks[start_chunk:end_chunk]):\n",
                "    current_idx = start_chunk + i\n",
                "    try:\n",
                "        questions_text = chain_openAI.invoke({\"context\": chunk})\n",
                "        qa_output = chain_groq.invoke({\"context\": chunk, \"questions\": questions_text})\n",
                "        \n",
                "        dataset.append({\n",
                "            \"chunk_id\": current_idx,\n",
                "            \"context\": chunk['page_content'],\n",
                "            \"qa_pairs\": qa_output\n",
                "        })\n",
                "        \n",
                "        if (current_idx + 1) % 10 == 0:\n",
                "            with open(output_file, \"w\") as f:\n",
                "                json.dump(dataset, f, indent=4)\n",
                "            print(f\"Saved checkpoint at chunk {current_idx + 1}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error on chunk {current_idx}: {e}\")\n",
                "        break \n",
                "\n",
                "\n",
                "with open(output_file, \"w\") as f:\n",
                "    json.dump(dataset, f, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2986730",
            "metadata": {},
            "source": [
                "### 5. Loading Q & A pairs and Splitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "513d55f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_file = \"../artifacts/data_factory/uber_dataset.json\"\n",
                "import json\n",
                "import re\n",
                "import random\n",
                "\n",
                "# Load raw data\n",
                "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
                "    raw_data = json.load(f)\n",
                "\n",
                "expanded_dataset = []\n",
                "\n",
                "for item in raw_data:\n",
                "    context = item[\"context\"]\n",
                "    qa_text = item[\"qa_pairs\"]\n",
                "    \n",
                "\n",
                "    pairs = re.split(r'\\n?\\s?Q:', qa_text)\n",
                "    \n",
                "    for pair in pairs:\n",
                "        if not pair.strip(): continue\n",
                "        \n",
                "        # split question and the answer\n",
                "        if 'A:' in pair:\n",
                "            parts = pair.split('A:', 1)\n",
                "            question = parts[0].strip().replace('Q:', '')\n",
                "            answer = parts[1].strip()\n",
                "            \n",
                "            expanded_dataset.append({\n",
                "                \"context\": context,\n",
                "                \"question\": question,\n",
                "                \"answer\":answer\n",
                "            })\n",
                "\n",
                "# Shuffle the final expanded dataset\n",
                "random.shuffle(expanded_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "a9493c08",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 80/20 Split\n",
                "split_point = int(len(expanded_dataset) * 0.8)\n",
                "train_set = expanded_dataset[:split_point]\n",
                "test_set = expanded_dataset[split_point:]\n",
                "\n",
                "\n",
                "def save_jsonl(data, filename):\n",
                "    with open(filename, 'w', encoding='utf-8') as f:\n",
                "        for entry in data:\n",
                "            f.write(json.dumps(entry) + '\\n')\n",
                "\n",
                "save_jsonl(train_set, \"../artifacts/train/train.jsonl\")\n",
                "save_jsonl(test_set, \"../artifacts/test/golden_test_set.jsonl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8088054",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# with open(\"readable_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "#     json.dump(train_set, f, indent=4, ensure_ascii=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "700a8552",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
